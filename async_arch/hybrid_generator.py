import jinja2
import asyncio
from typing import Dict, Any

# Mock Template Loader - In production, use FileSystemLoader('templates')
template_loader = jinja2.DictLoader({
    "flask_service.py.j2": """
from flask import Flask, request, jsonify
import logging

# Standard Configuration (Generated by Template)
app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

@app.route('/health')
def health_check():
    return jsonify({"status": "healthy", "service": "{{ service_name }}"})

# Business Logic Endpoint
@app.route('/execute', methods=['POST'])
def execute_task():
    \"\"\"
    {{ task_description }}
    \"\"\"
    try:
        data = request.json
        # TODO: AI_IMPLEMENTATION_NEEDED
        # The agent should implement the logic based on task_description
        pass
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(port={{ port }}, debug=True)
"""
})

env = jinja2.Environment(loader=template_loader)

class HybridGenerator:
    def __init__(self, llm_client=None):
        self.llm_client = llm_client 

    async def generate_module(self, template_name: str, context: Dict[str, Any]) -> str:
        """
        Hybrid Generation Process:
        1. Render structural boilerplate using Jinja2 (Fast, Error-free).
        2. Identify 'holes' marked for AI implementation.
        3. Invoke LLM to fill specifically those holes (Focused, Creative).
        """
        print(f"[TEMPLATE] Rendering template: {template_name}")
        
        # Step 1: Template Rendering
        template = env.get_template(template_name)
        # Render the safe, boilerplate parts
        initial_code = template.render(**context)
        
        # Step 2: AI Injection
        if "TODO: AI_IMPLEMENTATION_NEEDED" in initial_code:
            print("[HYBRID] AI Injection point found. Calling Specialist Agent...")
            final_code = await self.fill_logic_gap(initial_code, context)
            return final_code
        
        return initial_code

    async def fill_logic_gap(self, code: str, context: Dict) -> str:
        """
        Sends the code context to an LLM to fill the specific logic block.
        """
        # Here you would construct the prompt for the LLM
        # prompt = f"Complete the python code below. Replace 'pass' with logic for: {context['task_description']}..."
        
        # Simulating Async LLM Call
        await asyncio.sleep(1) 
        
        # Mocked AI Response
        # In a real scenario, the LLM would insert the actual string "PaymentProcessor" 
        # or use a variable available in the scope.
        # Here we simulate the LLM using the known service name.
        service_name = context.get('service_name', 'UnknownService')
        
        ai_logic = f"""
        # AI Generated Logic
        if not data.get('payload'):
            raise ValueError("Missing payload")
            
        result = f"Processed {{data['payload']}} via {service_name}"
        logging.info(f"Task executed: {{result}}")
        return jsonify({{"result": result}})
        """
        
        # Replace the marker and 'pass' with actual logic
        final_code = code.replace("pass", ai_logic.strip())
        final_code = final_code.replace("# TODO: AI_IMPLEMENTATION_NEEDED", "# Logic implemented by Agent Developer")
        
        return final_code

# Entry point for testing
async def main():
    generator = HybridGenerator()
    
    ctx = {
        "service_name": "PaymentProcessor",
        "port": 8080,
        "task_description": "Process credit card payments and return transaction ID"
    }
    
    code = await generator.generate_module("flask_service.py.j2", ctx)
    print("\n--- GENERATED HYBRID CODE ---\n")
    print(code)

if __name__ == "__main__":
    asyncio.run(main())
